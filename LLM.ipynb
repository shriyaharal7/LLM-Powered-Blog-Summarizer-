{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# LLM-Powered Blog Summarizer: Exploring Cutting-Edge Techniques\n",
        "In today's world of too much online information, it's important to quickly understand long blog posts. That's where Language Model (LLM) technologies come in. They're changing how we read and shorten big amounts of text. This project, called \"LLM-Powered Blog Summarizer: Exploring Cutting-Edge Techniques,\" looks into these advanced models.\n",
        "\n",
        "We start by picking out useful information from long blog posts. Using top LLMs like BART, T5, and Pegasus, we show how they can summarize blogs accurately and efficiently.\n",
        "\n",
        "We use different LLMs, each with its own way of working, to make short summaries of long blogs. By using BART, T5, and Pegasus models, we want to see what works best and compare their results.\n",
        "\n",
        "Behind the scenes, we check how similar the summaries are to a reference using cosine similarity. This helps us see if the summarization techniques are working well.\n",
        "\n",
        "Come join us as we explore LLMs and see how they're changing how we summarize blogs. It's all about the latest techniques making blog reading easier."
      ],
      "metadata": {
        "id": "GiLmohb6Q4EX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 1: Extracting Blog Content\n",
        "Our first task is to read the content of a blog. We have a file called \"blog.txt\" where the blog is stored. Our goal here is to open the file, read its contents, and store them in a variable so we can work with them later. Let's go ahead and do that!"
      ],
      "metadata": {
        "id": "MPFJrM_WQ9Vi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y-XMZXZHQ19J"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import string\n",
        "\n",
        "file_path = 'blog.txt'\n",
        "\n",
        "with open(file_path, 'r', encoding='utf-8') as file:\n",
        "    blog_content = file.read()\n",
        "\n",
        "blog_content\n",
        "\n",
        "#--- Inspect Blog txt ---"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 2: Cleaning Blog TextÂ¶\n",
        "Great job on reading the blog content! Now, let's clean up the text to make it easier to work with. We'll convert all the text to lowercase so that we don't have to worry about capitalization. Then, we'll remove any extra spaces between paragraphs to make the text more uniform. This will help us analyze the blog more effectively. Let's clean up the text and get it ready for analysis!"
      ],
      "metadata": {
        "id": "OaTO29fWRFBa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Remove special characters, numbers, and URLs\n",
        "cleaned_text = re.sub(r'[^\\w\\s]', ' ', blog_content)\n",
        "cleaned_text = re.sub(r'\\d+', '', cleaned_text)\n",
        "cleaned_text = re.sub(r'http\\S+', '', cleaned_text)"
      ],
      "metadata": {
        "id": "cvkzsPO6Q5Z8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 3: Summarizing Blog Content\n",
        "Well done on cleaning up the blog text! Now, let's take it a step further and create a summary of the blog. We'll use a special tool called BART to help us with this task. First, we'll load the BART tokenizer and model. Then, we'll tokenize the cleaned text and feed it into the model to generate a summary. The summary will capture the main points of the blog in a concise form. Once generated, we'll save the summary to a file for future reference. Let's generate the summary and see what insights we can gather from the blog!\n",
        "\n",
        "Cosine Similarity for Text Summarization Testing:\n",
        "\n",
        "Cosine similarity is a way to see how much alike two sets of things are. In text summarization, it's used to check how similar a summary made by a computer is to one made by a person. It looks at the direction of the sets, not how big they are. This method works well for understanding if two texts are similar in meaning. It doesn't matter how long the texts are. Using cosine similarity helps us measure how good a summarization model is at making summaries that match the original text.\n",
        "\n",
        "Please execute the following command to install the transformer library. Once installed, restart the kernel and run all cells in the notebook."
      ],
      "metadata": {
        "id": "b2z0J0OBRNPY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers"
      ],
      "metadata": {
        "id": "1tnCDJqQRJNn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the necessary modules\n",
        "from transformers import BartTokenizer, TFBartForConditionalGeneration\n",
        "\n",
        "# Load the tokenizer and model\n",
        "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "model = TFBartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "\n",
        "# Tokenize the cleaned text\n",
        "inputs = tokenizer(cleaned_text, return_tensors=\"tf\")\n",
        "\n",
        "# Generate the summary\n",
        "summary_ids = model.generate(inputs[\"input_ids\"], num_beams=4, max_length=150, early_stopping=True)\n",
        "\n",
        "# Decode the summary\n",
        "summary_bart = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "# Print the generated summary\n",
        "print(summary_bart)\n",
        "\n",
        "# Export the generated summary to a file\n",
        "output_filepath = \"summary_bart.txt\"\n",
        "with open(output_filepath, \"w\", encoding=\"utf-8\") as output_file:\n",
        "    output_file.write(summary_bart)\n"
      ],
      "metadata": {
        "id": "lO-BTneRRTFW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 4: Alternative Blog Summarization with T5 Model\n",
        "Fantastic job on generating the blog summary using BART! Now, let's explore another approach to summarizing the blog content. We'll use a different tool called T5 to help us with this task. Similar to before, we'll load the T5 tokenizer and model. Then, we'll tokenize the cleaned text and feed it into the model to generate a summary. Once generated, we'll save the summary to a file for future reference. Let's generate the summary and see how it compares to the one we created earlier!\n",
        "\n",
        "Please execute the following command to install the SentencePiece library. Once installed, restart the kernel and run all cells in the notebook."
      ],
      "metadata": {
        "id": "-JNDy99AReZ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install sentencePiece"
      ],
      "metadata": {
        "id": "ZL1vnjlgRaz_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the necessary modules\n",
        "from transformers import BartTokenizer, TFBartForConditionalGeneration\n",
        "\n",
        "# Load the tokenizer and model\n",
        "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "model = TFBartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "\n",
        "# Tokenize the cleaned text\n",
        "inputs = tokenizer(cleaned_text, return_tensors=\"tf\")\n",
        "\n",
        "# Generate the summary\n",
        "summary_ids = model.generate(inputs[\"input_ids\"], num_beams=4, max_length=150, early_stopping=True)\n",
        "\n",
        "# Decode the summary\n",
        "summary_t5 = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "# Print the generated summary\n",
        "print(summary_t5)\n",
        "\n",
        "# Export the generated summary to a file\n",
        "output_filepath = \"summary_t5.txt\"\n",
        "with open(output_filepath, \"w\", encoding=\"utf-8\") as output_file:\n",
        "    output_file.write(summary_t5)"
      ],
      "metadata": {
        "id": "_C_2bX_NRjf0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 5: Employing Pegasus Model for Blog Summarization\n",
        "Well done on exploring multiple approaches to summarizing the blog content! Now, let's try yet another method using Pegasus. We'll load the Pegasus tokenizer and model to help us with this task. Following a similar process as before, we'll tokenize the cleaned text and feed it into the model to generate a summary. The summary will give us a condensed version of the blog's key points. Once generated, we'll save the summary to a file for future reference. Let's generate the summary and see how it differs from the previous ones!"
      ],
      "metadata": {
        "id": "J7TaLJYvRo0n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the necessary modules\n",
        "from transformers import PegasusTokenizer, TFPegasusForConditionalGeneration\n",
        "\n",
        "# Load the tokenizer and model\n",
        "tokenizer = PegasusTokenizer.from_pretrained(\"google/pegasus-large\")\n",
        "model = TFPegasusForConditionalGeneration.from_pretrained(\"google/pegasus-large\")\n",
        "\n",
        "# Tokenize the cleaned text\n",
        "inputs = tokenizer(cleaned_text, return_tensors=\"tf\", padding=\"max_length\", truncation=True, max_length=512)\n",
        "\n",
        "# Generate the summary\n",
        "summary_ids = model.generate(inputs[\"input_ids\"], num_beams=4, max_length=150, early_stopping=True)\n",
        "\n",
        "# Decode the summary\n",
        "summary_pegasus = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "# Print the generated summary\n",
        "print(summary_pegasus)\n",
        "\n",
        "# Export the generated summary to a file\n",
        "output_filepath = \"summary_pegasus.txt\"\n",
        "with open(output_filepath, \"w\", encoding=\"utf-8\") as output_file:\n",
        "    output_file.write(summary_pegasus)"
      ],
      "metadata": {
        "id": "zPmBaMyJRmfr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}